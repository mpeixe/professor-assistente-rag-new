import os
import streamlit as st
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda

# --- Fun√ß√µes de processamento de RAG ---

def setup_rag_system():
    """
    Configura e retorna a cadeia de RAG com m√∫ltiplos √≠ndices.
    Esta fun√ß√£o deve ser executada apenas uma vez.
    """
    # Use a chave de API do OpenRouter
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        st.error("Chave de API do OpenRouter (OPENROUTER_API_KEY) n√£o encontrada. Por favor, configure a vari√°vel de ambiente.")
        st.stop()

    # Estrutura para os dados das disciplinas
    disciplinas_data = {
        "biologia": {
            "url": "https://pt.wikipedia.org/wiki/Biologia_celular",
        },
        "fisica": {
            "url": "https://pt.wikipedia.org/wiki/F%C3%ADsica_qu%C3%A2ntica",
        }
    }
    
    # Configure a base URL para o OpenRouter
    # Nota: A LangChain com OpenAI ainda usa o nome da vari√°vel OPENAI_API_KEY por padr√£o,
    # ent√£o usamos o nome da vari√°vel do OpenRouter e passamos a API Key e o base_url
    # para a inst√¢ncia do ChatOpenAI.
    os.environ["OPENAI_API_KEY"] = api_key
    base_url = "https://openrouter.ai/api/v1"

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    
    # A Langchain ainda usa a classe OpenAIEmbeddings e ChatOpenAI, mas
    # √© poss√≠vel passar o par√¢metro base_url para o construtor
    embeddings = OpenAIEmbeddings(openai_api_base=base_url)

    # Cria e armazena os √≠ndices em cache
    for disciplina, data in disciplinas_data.items():
        index_name = f"faiss_index_{disciplina}"
        if not os.path.exists(index_name):
            with st.spinner(f"Criando √≠ndice de {disciplina.capitalize()}..."):
                loader = WebBaseLoader(data["url"])
                documents = loader.load()
                docs = text_splitter.split_documents(documents)
                vectorstore = FAISS.from_documents(docs, embeddings)
                vectorstore.save_local(index_name)
        else:
            with st.spinner(f"Carregando √≠ndice de {disciplina.capitalize()}..."):
                vectorstore = FAISS.load_local(index_name, embeddings, allow_dangerous_deserialization=True)
        data["vectorstore"] = vectorstore

    # --- Roteador para classificar a pergunta ---
    llm_classifier = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, openai_api_base=base_url)
    prompt_classificador = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© um assistente de roteamento que classifica perguntas sobre diferentes disciplinas.
        Sua tarefa √© identificar a qual disciplina a pergunta pertence. As disciplinas s√£o: biologia, fisica.
        Responda APENAS com o nome da disciplina, em letras min√∫sculas.
        Se n√£o se encaixar, responda 'outros'."""),
        ("human", "{input}")
    ])
    classificacao_chain = prompt_classificador | llm_classifier

    # --- Cadeia de resposta (gen√©rica) ---
    llm_responder = ChatOpenAI(temperature=0, openai_api_base=base_url)
    prompt_resposta = ChatPromptTemplate.from_template("""
    Responda √† pergunta do usu√°rio usando apenas o contexto fornecido.
    Contexto: {context}
    Pergunta: {input}
    """)
    document_chain = create_stuff_documents_chain(llm_responder, prompt_resposta)

    return classificacao_chain, document_chain, disciplinas_data

def get_answer(question, classificacao_chain, document_chain, disciplinas_data):
    """
    Roteia a pergunta, busca no √≠ndice correto e gera a resposta.
    """
    # 1. Classificar a pergunta
    disciplina_content = classificacao_chain.invoke({"input": question}).content
    disciplina = disciplina_content.strip().lower()

    if disciplina in disciplinas_data:
        # 2. Obter o retriever correto
        vectorstore = disciplinas_data[disciplina]["vectorstore"]
        retriever = vectorstore.as_retriever()

        # 3. Criar a cadeia de RAG espec√≠fica para a disciplina
        retrieval_chain = create_retrieval_chain(retriever, document_chain)

        # 4. Invocar a cadeia e obter a resposta
        response = retrieval_chain.invoke({"input": question})
        return response["answer"], disciplina
    else:
        return "Desculpe, a pergunta n√£o se encaixa em nenhuma das disciplinas que conhe√ßo (biologia, f√≠sica).", None

# --- Interface do Streamlit ---

st.set_page_config(page_title="Professor Assistente RAG", layout="wide")

st.title("üë®‚Äçüè´ Professor Assistente RAG")
st.write("Pergunte sobre Biologia ou F√≠sica e obtenha respostas baseadas em conhecimento especializado.")

# Inicializar o sistema RAG uma √∫nica vez
if "rag_system" not in st.session_state:
    st.session_state.rag_system = setup_rag_system()

classificacao_chain, document_chain, disciplinas_data = st.session_state.rag_system

# Entrada do usu√°rio
user_question = st.text_input("Digite sua pergunta:", placeholder="Ex: O que √© uma c√©lula eucariota?")

if user_question:
    with st.spinner("Gerando resposta..."):
        answer, disciplina = get_answer(user_question, classificacao_chain, document_chain, disciplinas_data)

    if disciplina:
        st.markdown(f"**Identifiquei que sua pergunta √© sobre:** *{disciplina.capitalize()}*")
    st.success(answer)